#--------------------------------------------------------------------------------------------
# Wish list 
#	- mkdir if folder for -f doesn't exist. 
#	- don't say 'Enjoy the video' when there is no download. 
#	- maintain a trace and don't redownload what you have already got. { verify in the directory ... } 
#	- where and how to keep the trace? what is the file format? 
#	- maintain csv generator that can keep the trace. 
#	- downalod_stream function should provide data of actual status and details of download (put it to trace)
#	- you can also check if the file is physically present. 
# 	- skip what is already downloaded  { verify in the directory ... } 
#	- get watch page to retrive the play duration - dont' need to downloard the full thing for it. 
#	- needs to see that duration available locally matches with what is claimed by YT { to verify if the file downloaded is complete! } 
#	- MD5 or play duration verification of Media files (large ones) 
#	- deal with network failures. -- retry, wait-n-resume
#	- deal with resume on socket. (resume partial downloads) 
#	- Resume incomplete downloads from where they were left 
#	- capture critical stuff and do clean exit when user presses Ctr+C 
#	- list control - follow list, pause, stop and resume post what is finished. 
#	- make it interactive shell? i.e. commands can be given while work is going on. Press Ctr+C to interrupt
#	- parallel thread downloading 
#	- generalize the output file format ... make it what user wants. Can be done at FFMPEG level. 
#	- network reconnect should re-establish things
#	- ask if the file to be downloaded already exists
#	- probe for file size in yl format? 
#	- fetch can be directly given -p/-c/-u (for playlist,channel,user respectively) without calling extract.
#	- instead of pretty print. Video meta can be explained in more human readable format. 
#	- generic methods/ or customzable file titles. 
#	- generic policies in what to fetch : AudioOnly/VideoOnly/AV, Max res, 
#	- post processing of video. e.g. convert to mp3
#	- in case of multiple parallel downloads, the current \r funda won't work. So need central reporting engine.
#	- If youtube throws 404/403 - in actual downloads. it should not proceed like a normal flow. Exception should be handled. [ Done! ]
#	- Deal with signatured streams that otherwise throws 403 (e.g. '-RMD88DNaGk') 
#	- there exists custom urls for channels. see https://www.youtube.com/account_advanced
#	- my own channel is different from std (makemegenious) https://www.youtube.com/channel/UCzvGqEme9-yngxiYXY_Zk7A
#	- tabs being compulsory in .yl files is a problem, specially when the files are handwritten. 
#	- fetch can now be given multiple uid_references which it can download all serially
#	- now the .yl in turn can have references to full channel, playlist etc. expand all of them before download 
# 	- file issues for well defined items. 
#	- list should now be generlized and should be out from extract into ytlist
#	- list should be able to deal with all types of lists : channel, user, playlist, ytlist
#	- prepare readme - specially for explaining usage and download_ref. 
#	- specify dependencies -e.g. certifi 
#----------------------------------------------------------------------------------------------------
# Code TODO 
#	- make item logger global (how will you handle in multithread situation?) 
#	- centrlized all standard strings (including youtube.com/xxx where we are assuming known url 
#	- get rid of print_smap_XXX functions and no extra return value from parse_stream_map 
#	- keep all common strings in a central place 
#	- it's time, the code must have classes for important stuff. 
#	- geting page, streams and captions all have different types of download functions. Generalize!
#	- create your own powerful downloader which can go chunk-by-chunk with better control. 
#	- load_list of different type should come with simple and single abstraction. 
#-----------------------------------------------------------------------------------------------------


